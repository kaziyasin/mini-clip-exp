# Mini CLIP Experiment ğŸ¥ğŸ§ 
*A 365-day journey to build my own small multimodal AI model from scratch.*

## ğŸš€ Goal
This project documents my daily/weekly/monthly progress as I learn and build a tiny CLIP-style model that connects **images + text**.  
The goal is not to create a state-of-the-art model, but to deeply understand:
- Vision transformers  
- Text encoders  
- Contrastive learning  
- Dataset curation  
- Training pipelines  
- Evaluation & benchmarks  
- Scaling laws and training stability  

By the end of this journey, I want to reach a level where I am equipped to operate as a **frontier researcher** in multimodal AI.

## ğŸ“… The Plan (Short Summary)
- **Daily:** Learn something + code something small.  
- **Weekly:** Add one feature, dataset, or experiment.  
- **Monthly:** Produce one measurable result (plot, benchmark, or demo).  
- **1 Year:** A full working tiny-CLIP experiment with clean code + training notes.

## ğŸ“š Why Iâ€™m Doing This
I want to train myself into a **deep AI expert**â€”the kind who can join teams working on frontier models.  
This repository keeps me accountable, transparent, and consistent.

## ğŸ¬ Recording My Learning
I will also document the entire journey on:
- **YouTube Shorts / Faceless Videos**
- **Skool Community** (â€œ365 Days of Building AI with KYIâ€)

## ğŸ§© Repo Structure (initial skeleton)
```
mini-clip-kyi
â”œâ”€â”€ data/
â”œâ”€â”€ models/
â”œâ”€â”€ training/
â”œâ”€â”€ utils/
â””â”€â”€ README.md
```

## ğŸ“ Week 1 Progress
- Created project structure  
- Planned the 365-day roadmap  
- First learning session: Karpathy video â€œLet's build GPT: from scratch, in code, spelled out" https://www.youtube.com/watch?v=kCc8FmEb1nY
